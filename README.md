# ğŸš€ Enterprise Hybrid Graph-RAG System

An enterprise-grade Hybrid Retrieval-Augmented Generation (Graph-RAG) system combining multi-hop knowledge graph reasoning, semantic vector retrieval, weighted ranking, evaluation metrics, latency instrumentation, and Amazon Bedrock Claude integration.

This project focuses on system design, explainability, evaluation rigor, and production readiness â€” moving beyond traditional vector-only RAG pipelines.

---

## ğŸ§  Architecture Overview

This system integrates:

- Multi-hop Knowledge Graph reasoning (Amazon Neptune compatible)
- FAISS-based semantic vector retrieval
- Weighted hybrid ranking (graph + semantic scoring)
- Confidence calibration
- Retrieval evaluation metrics
- Latency instrumentation (stage-wise breakdown)
- Amazon Bedrock Claude 3 for grounded answer synthesis
- Modular domain-driven architecture

---

## ğŸ§  End-to-End Pipeline

```text
User Query
    â†“
Entity Extraction
    â†“
Multi-Hop Graph Expansion
    â†“
Vector Similarity Retrieval (FAISS)
    â†“
Hybrid Ranking (Graph + Semantic)
    â†“
Structured Prompt Construction
    â†“
Claude 3 (Amazon Bedrock)
    â†“
Grounded Answer
```

This architecture enforces separation between retrieval and generation while preserving explainability and observability.

---

## ğŸ”¥ LLM Integration (Amazon Bedrock)

The system integrates Amazon Bedrock (Claude 3 Sonnet) for grounded answer generation.

Key design principles:

- Evidence-constrained prompting
- Knowledge graph + document context injection
- Strict grounding instructions to reduce hallucination
- Retrieval â†’ Generation separation
- Model-agnostic LLM abstraction layer

LLM calls are made via AWS Bedrock Runtime using `boto3`.

---

## ğŸ” Hybrid Scoring Strategy

Final ranking score:

```
final_score =
    Î± * normalized_vector_score
  + Î² * graph_path_score
```

Graph scoring incorporates:

- Hop-based decay
- Path weighting
- Deduplicated reasoning paths
- Multi-entity expansion

This ensures structural reasoning complements semantic similarity.

---

## ğŸ“Š Retrieval Evaluation Framework

Implemented metrics:

- Precision@K
- Recall@K
- Mean Reciprocal Rank (MRR)
- Graph Coverage Score

This enables measurable retrieval quality rather than heuristic-only evaluation.

---

## âš¡ Latency Instrumentation

Stage-wise performance tracking:

- Graph expansion latency
- Vector search latency
- Ranking latency
- Total end-to-end latency

Example:

```json
"latency": {
  "total_ms": 42.82,
  "graph_ms": 18.75,
  "vector_ms": 24.05,
  "ranking_ms": 0.02
}
```

This enables observability and performance optimization.

---

## ğŸ“ˆ Sample Output

Below is an example of structured, explainable output returned by the Hybrid Retrieval Engine:

```json
{
  "query": "How does Amazon Neptune integrate with Bedrock?",
  "entities": [
    "Amazon Neptune",
    "Bedrock"
  ],
  "top_result": {
    "text": "Amazon Neptune integrates with Bedrock for AI applications.",
    "vector_score": 0.0,
    "graph_score": 1.0,
    "final_score": 0.4
  },
  "confidence": 0.4,
  "latency": {
    "total_ms": 42.82,
    "graph_ms": 18.75,
    "vector_ms": 24.05,
    "ranking_ms": 0.02
  },
  "graph_paths": [
    {
      "nodes": ["Amazon Neptune", "Amazon Bedrock"],
      "hop_count": 1,
      "score": 1.0
    },
    {
      "nodes": ["Amazon Neptune", "Bedrock"],
      "hop_count": 1,
      "score": 1.0
    }
  ]
}
```

### ğŸ” What This Demonstrates

- Explicit reasoning paths (multi-hop graph expansion)
- Hybrid ranking transparency (graph + semantic scoring)
- Confidence calibration
- Stage-wise latency instrumentation
- Structured output suitable for production APIs
- Grounded answer generation using Bedrock Claude

This architecture prioritizes explainability, measurable retrieval quality, and performance observability.

---

## ğŸ— Project Structure

```
graph_rag/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ vector/
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ ranking/
â”‚   â”œâ”€â”€ llm/
â”‚   â””â”€â”€ evaluation/
â”‚
â”œâ”€â”€ ingestion/
â”œâ”€â”€ config/
â”œâ”€â”€ tests/
```

The system follows modular domain separation:

- Retrieval layer
- Ranking layer
- LLM layer
- Evaluation layer
- Observability layer

---

## ğŸ›  Technologies Used

- Python
- FAISS
- Amazon Neptune (Gremlin)
- AWS Bedrock
- Claude 3 Sonnet
- boto3
- Pydantic
- Docker-ready structure

---

## ğŸ¯ Key Differentiators

Unlike standard RAG implementations, this system:

- Combines structured graph reasoning with semantic retrieval
- Produces explainable reasoning traces
- Implements measurable evaluation metrics
- Instruments latency across pipeline stages
- Separates retrieval from generation
- Integrates cloud-native LLM infrastructure

---

## ğŸš€ Future Enhancements

- FastAPI production API
- Docker containerization
- CI/CD pipeline
- Benchmark dataset integration
- Cost and token usage tracking
- Multi-model support abstraction

---

## ğŸ“Œ About

Enterprise Hybrid Graph-RAG system with multi-hop reasoning, weighted hybrid ranking, evaluation metrics, latency instrumentation, and Amazon Bedrock Claude integration.
